{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a4de95a-2db5-40c2-ab25-f6f0979815ce",
      "metadata": {
        "id": "3a4de95a-2db5-40c2-ab25-f6f0979815ce"
      },
      "source": [
        "## IT 720: NLP, Assignment 2\n",
        "\n",
        "When you submit the assignment, make sure that you:\n",
        "\n",
        "- add your name to the file name\n",
        "- shut down the kernel one last time, restart it, and run your code from start to finish.\n",
        "- leave the output in each cell to allow the grader to see it\n",
        "- if there is a bug that you cannot resolve, leave the error message in the output cell so the grader can see it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a376d9-b034-4ba7-a7e4-097acd62ce76",
      "metadata": {
        "id": "65a376d9-b034-4ba7-a7e4-097acd62ce76"
      },
      "source": [
        "### Rubric for the numbered sections below where you must write your own code, 175 points total\n",
        "1.  10 points: Train, Valid, Test Split\n",
        "2.  20 points: BiLSTM\n",
        "3.  20 points: BiLSTM with Classifier Head\n",
        "4.  20 points: Fit Encoder\n",
        "5.  20 points: Visualize Training Loss and Accuracy\n",
        "6.  20 points: Create Sentence Embeddings\n",
        "7.  20 points: Define and Train FFClassifier\n",
        "8.  15 points: Combine Data and Retrain\n",
        "9.  10 points: Confusion Matrix\n",
        "10. 20 points: Two UMAP Scatterplots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf4eafcc-c98c-4101-89e2-c0257f61df02",
      "metadata": {
        "id": "cf4eafcc-c98c-4101-89e2-c0257f61df02"
      },
      "source": [
        "### Assignment 2 – Neural Sentence Embeddings with BiLSTM + Feedforward Classifier\n",
        "\n",
        "#### The first 3 code cells are provided to help you get started, as well a few other cells below.\n",
        "\n",
        "In this assignment, your code will:\n",
        "\n",
        "- Build a neural **sentence encoder** using a bi‑directional LSTM\n",
        "- Learn the word (token) embeddings\n",
        "- Use the encoder to generate **fixed‑dimensional embeddings** for movie reviews.\n",
        "- Train a **feed‑forward neural network (FFNN)** classifier on top of these embeddings.\n",
        "- Evaluate performance and conduct **error analysis** on misclassified examples.\n",
        "\n",
        "This solution notebook contains:\n",
        "\n",
        "1. Data loading and (minimal) preprocessing  \n",
        "2. BiLSTM sentence encoder construction and training  \n",
        "3. Embedding extraction for all reviews  \n",
        "4. Feed‑forward classifier training\n",
        "5. Evaluation and confusion matrix  \n",
        "6. Error analysis with concrete examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d17ba0-9372-4249-84f8-f8e612ff794d",
      "metadata": {
        "id": "92d17ba0-9372-4249-84f8-f8e612ff794d"
      },
      "outputs": [],
      "source": [
        "# This cell contains all packages and functions you need to do the assignment.\n",
        "# You may substitute PyTorch code instead of Keras/Tensorflow, but you would\n",
        "# have to replace many imports here with similar imports for PyTorch.\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "# !pip install umap-learn --quiet  # One time installation\n",
        "import umap # We use this to visualize the movie review vector space\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy             as np\n",
        "import seaborn           as sns\n",
        "import tensorflow        as tf\n",
        "\n",
        "\n",
        "from nltk                    import word_tokenize\n",
        "from nltk.corpus             import movie_reviews\n",
        "from sklearn.metrics         import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.callbacks              import EarlyStopping\n",
        "from tensorflow.keras.layers                 import Input, Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models                 import Model, Sequential\n",
        "from tensorflow.keras.optimizers             import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text     import Tokenizer\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51f86af-2e9b-4e59-9542-4da852407f07",
      "metadata": {
        "id": "e51f86af-2e9b-4e59-9542-4da852407f07"
      },
      "source": [
        "## Reproducibility and setup\n",
        "\n",
        "We set random seeds for `numpy`, Python's `random`, and TensorFlow to reduce variance in results.  \n",
        "Note: in real training, results will still vary slightly across runs and hardware, but this helps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091d8b5b-969c-441a-9eb1-c928a758f80f",
      "metadata": {
        "id": "091d8b5b-969c-441a-9eb1-c928a758f80f"
      },
      "outputs": [],
      "source": [
        "SEED = 34\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# The next line is for GPU determinism (may not be perfect across all environments)\n",
        "# If this generates an error, just delete it. It's optional.\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b085b0d-b053-4812-8bd1-f55dc50b3188",
      "metadata": {
        "id": "3b085b0d-b053-4812-8bd1-f55dc50b3188"
      },
      "source": [
        "## Loading the movie reviews corpus\n",
        "\n",
        "We reuse the same **NLTK `movie_reviews`** dataset as in Assignment 1.\n",
        "\n",
        "- Each document is a full review.\n",
        "- Each review is labeled with a **sentiment**: `pos` or `neg`.\n",
        "- We will:\n",
        "  - Extract raw tokenized reviews.\n",
        "  - Convert them into text sequences (space‑joined tokens).\n",
        "  - Assign integer labels: `0` for negative, `1` for positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadd16e3-3e3d-4325-96bb-09b16c18a84b",
      "metadata": {
        "id": "aadd16e3-3e3d-4325-96bb-09b16c18a84b"
      },
      "outputs": [],
      "source": [
        "# Load the movie reviews\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# movie_reviews.fileids() gives list like ['neg/cv000_29416.txt', 'pos/cv000_29590.txt', ...]\n",
        "fileids = movie_reviews.fileids()\n",
        "print(\"Number of documents:\", len(fileids))\n",
        "print(\"Example fileid:\", fileids[0])\n",
        "\n",
        "# Extract documents and labels\n",
        "documents = []\n",
        "labels    = []    # We will add 0 for negative review and 1 for positive\n",
        "\n",
        "for fid in fileids:\n",
        "    # tokens: list of strings\n",
        "    tokens = movie_reviews.words(fid)\n",
        "    # join back into a whitespace-separated string\n",
        "    text = \" \".join(tokens)\n",
        "    documents.append(text)\n",
        "\n",
        "    # label: 'pos' or 'neg'\n",
        "    label = movie_reviews.categories(fid)[0]\n",
        "    labels.append(1 if label == \"pos\" else 0)\n",
        "\n",
        "print(\"\\nExample document:\")\n",
        "print(documents[0][:500], \"...\")\n",
        "print(\"\\nLabel (0=neg, 1=pos):\", labels[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb46f44-1af5-4b55-97f7-b7d119cb2076",
      "metadata": {
        "id": "7cb46f44-1af5-4b55-97f7-b7d119cb2076"
      },
      "source": [
        "# Your own code will begin here.\n",
        "\n",
        "## 1. Train/validation/test split\n",
        "\n",
        "Add your own code to split the dataset into:\n",
        "\n",
        "- **Train**: for fitting the BiLSTM encoder and the FFNN.\n",
        "- **Validation**: for early stopping during BiLSTM training.\n",
        "- **Test**: held‑out data for final evaluation.\n",
        "\n",
        "Using scikit-learn's train_test_split, create an 80-20 split for training and test data, but then do another 80-20 split on that training data to get a training set with 64% of the original 2000 reviews with 16% of the 2000 held out for validation.\n",
        "\n",
        "Please create the following variable names in your code for the training, validation and test sets:\n",
        "- X_train, y_train\n",
        "- X_val,   y_val\n",
        "- X_test,  y_test\n",
        "\n",
        "After you have set the variables print out the shapes of X_train, X_val and X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdaa0bc1-22a9-4142-a6c6-13a8cb032249",
      "metadata": {
        "id": "cdaa0bc1-22a9-4142-a6c6-13a8cb032249"
      },
      "outputs": [],
      "source": [
        "X = np.array(documents)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Use X and y to now create your six variables for training, validation and test data.\n",
        "\n",
        "# First split: train+val vs test\n",
        "\n",
        "\n",
        "# Second split: train vs val\n",
        "\n",
        "\n",
        "# Print out shapes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cfb5601-5ea7-4cf9-8c7c-5985b151138f",
      "metadata": {
        "id": "4cfb5601-5ea7-4cf9-8c7c-5985b151138f"
      },
      "source": [
        "## Tokenization and integer encoding\n",
        "\n",
        "We now:\n",
        "\n",
        "1. Fit a Keras `Tokenizer` on the training text.\n",
        "2. Convert texts to integer sequences.\n",
        "3. Pad sequences to a fixed maximum length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af29af1f-856c-4bfb-99fb-32d02056eaa3",
      "metadata": {
        "id": "af29af1f-856c-4bfb-99fb-32d02056eaa3"
      },
      "outputs": [],
      "source": [
        "# This cell is provided to you and shows how to create and use a simple\n",
        "# whole-word based tokenizer using Keras' Tokenizer utility. The Keras\n",
        "# documentation is not very good regarding tokenizers. However, the following\n",
        "# HuggingFace page does provide good detail on several approaches to\n",
        "# tokenization, and is a good supplement to the somewhat limited discussion\n",
        "# that you can find in Jurafsky and Martin's chapter on 'Words and Tokens'\n",
        "# https://huggingface.co/docs/transformers/fast_tokenizers\n",
        "\n",
        "MAX_VOCAB_SIZE = 10000   # This will limit our total vocab size to most frequent 10000 in the data\n",
        "MAX_SEQ_LEN    = 256     # Truncate/pad to this length so every review is same length\n",
        "EMBEDDING_DIM  = 128     # Dimensionality of learned token embeddings\n",
        "\n",
        "# The next line will add '<OOV>' to the list of vocabulary words and assign it the integer 1.\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "\n",
        "# When fit_on_texts is called, it will find the top 9,999 words by frequency and assign a unique integer to each word.\n",
        "# Integer 1 has already been assigned to the special vocab token <OOV>, so any word found in the texts\n",
        "# that is not among those top 9,999 will just get assigned the 1.\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "def texts_to_padded_sequences(texts):\n",
        "    seqs = tokenizer.texts_to_sequences(texts)\n",
        "    return(pad_sequences(seqs, maxlen=MAX_SEQ_LEN, padding='post', truncating='post'))\n",
        "\n",
        "X_train_seq = texts_to_padded_sequences(X_train)\n",
        "X_val_seq   = texts_to_padded_sequences(X_val)\n",
        "X_test_seq  = texts_to_padded_sequences(X_test)\n",
        "\n",
        "print(\"Train sequences shape:\\t\\t\",    X_train_seq.shape)\n",
        "print(\"Validation sequences shape:\\t\", X_val_seq.shape)\n",
        "print(\"Test sequences shape:\\t\\t\",     X_test_seq.shape)\n",
        "\n",
        "# Each review has now been converted to a sequence of integers. Remember, 1 represents any low frequency word\n",
        "# which could be a misspelling or simply a very rare word.\n",
        "# Let's look at the tokenization of the first review in the training sequences:\n",
        "\n",
        "print('\\nThe tokenization of the first review in X_train_seq is:')\n",
        "X_train_seq[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2db9c0-681f-441b-b620-a0fd655352cc",
      "metadata": {
        "id": "8e2db9c0-681f-441b-b620-a0fd655352cc"
      },
      "source": [
        "## 2. Building the BiLSTM sentence encoder\n",
        "\n",
        "Build a Keras model that takes a sequence of token IDs and returns a **fixed‑dimensional sentence embedding**\n",
        "In doing so, the model will also learn embeddings for the tokens.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "- Input: integer sequence of length `MAX_SEQ_LEN`.\n",
        "- Embedding layer: maps tokens to `EMBEDDING_DIM`‑dimensional learned vectors.\n",
        "- BiLSTM: processes the sequence in both directions.\n",
        "- We set `return_sequences=False` to get a single vector per sequence.\n",
        "- A final dense layer to reshape the reviews dimension and add nonlinearity. This has been supplied for you.\n",
        "\n",
        "The final encoder outputs a sentence embedding vector of reduced dimensionality, i.e. fewer dimensions than output of the biLSTM.\n",
        "\n",
        "The shrinking of the final sentence embedding is because:\n",
        "a. Fewer dimensions may result in less overfitting due to the reduced capacity of representation. You will see\n",
        "   that overfitting may be a huge issue for this assignment, mainly because of the tiny size of the dataset\n",
        "   \n",
        "b. A 'bottleneck' of the  dimensionality for the sentence embeddings allows\n",
        "    you to make changes elsewhere and still arrive at the same final dimensionality\n",
        "    vis-a-vis the representation capacity of a biLSTM.\n",
        "    \n",
        "c. The tanh activation reshapes the embedding space into a bounded, centered region. This often\n",
        "   makes downstream classifiers behave more predictably\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53e726b-1084-472e-9da0-24162235e1e8",
      "metadata": {
        "id": "c53e726b-1084-472e-9da0-24162235e1e8"
      },
      "outputs": [],
      "source": [
        "# Build sequential LSTM model in Keras (or PyTorch, if you prefer).\n",
        "# You can choose your own hyperparameter values, e.g. number of units for the layers,\n",
        "# names of layers, etc. unless otherwise noted.\n",
        "\n",
        "# Begin with an Input layer that takes reviews with shape=(MAX_SEQ_LEN,)\n",
        "# Add an Embedding layer to learn word (token) embeddings\n",
        "#     with input dimensions of MAX_VOCAB_SIZE and output dimensions of EMBEDDING_DIM defined above\n",
        "# Add a bidirectional LSTM with return_sequences=False and that outputs 64 dimensions\n",
        "#     in each direction producing 128 dimensions for final LSTM output\n",
        "# Follow the biLSTM with a Dense layer that outputs only 48 dimensions. This is the 'bottleneck' mentioned above.\n",
        "# This Dense layer should use a 'tanh' activation function.\n",
        "# The output of this Dense layer will be the input to a small Classifier to perform the review classification\n",
        "# Give this Dense layer a name by using name=\"sentence_embedding\".  We will reference that name later.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5beb0fb1-cbf6-42fe-8920-2651d9faaf1e",
      "metadata": {
        "id": "5beb0fb1-cbf6-42fe-8920-2651d9faaf1e"
      },
      "source": [
        "## 3. Training the BiLSTM encoder for sentiment classification\n",
        "\n",
        "Before we train our biLSTM, we need to give it a supervisory signal for training that is relevant to our task of classifying movie review sentiments. Without that signal, the learned token embeddings will not be able to contribute useful information to the desired task.  Therefore, we will attach a simple classification head to our encoder solely for this purpose. Do not confuse the classification head here with the Feed Forward Classifier that we will create later after we train the encoder.  \n",
        "\n",
        "To train the encoder, we attach a classification head by simply following the sentence_embedding layer with:\n",
        "- Dropout layer for regularization, choosing your own value for the dropout rate.\n",
        "- Dense layer with 1 unit and sigmoid activation for binary sentiment.\n",
        "\n",
        "Make sure you have saved your entire encoder with its classifer head into a variable called 'biLSTM'.\n",
        "Compile biLSTM with \"binary_crossentropy\" for the loss function, the Adam optimizer and metrics=[\"accuracy\"]\n",
        "\n",
        "Call biLSTM.summary() to see a descrition of your complete encoder. You should see 6 layers in the output:\n",
        "1. Input\n",
        "2. Embedding\n",
        "3. Bidirectional\n",
        "4. Dense\n",
        "5. Dropout\n",
        "6. Dense\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2217fbb6-7e16-4c4f-9640-5105831ffcf8",
      "metadata": {
        "id": "2217fbb6-7e16-4c4f-9640-5105831ffcf8"
      },
      "outputs": [],
      "source": [
        "# Finish the rest of the encoder as just described.\n",
        "# Compile it and call biLSTM.summary() to view the results.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcf8952-2e10-45e8-a7b7-4b04e0749823",
      "metadata": {
        "id": "abcf8952-2e10-45e8-a7b7-4b04e0749823"
      },
      "source": [
        "## 4. Training with early stopping\n",
        "\n",
        "We train the model and use `EarlyStopping` on validation loss (`val_loss`) to try to prevent overfitting.\n",
        "Use `restore_best_weights=True` in the case that early stopping is triggered in order to have the best weights returned.\n",
        "\n",
        "Save the results of the training into the variable 'history', which we will use\n",
        "to plot the loss and accuracy curves below.\n",
        "\n",
        "Key hyperparameters are:\n",
        "\n",
        "- Batch size\n",
        "- Number of epochs\n",
        "- Learning rate (set above for Adam, if you wish to change the default)\n",
        "- Dropout rate (set above)\n",
        "- The `patience` for early stopping (i.e. how many epochs with no improvement in validation loss for early stopping)\n",
        "- Use `verbose=1` if you wish to see the output for each epoch during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a770df-0eb8-41c9-9e41-f2bdce0a74af",
      "metadata": {
        "id": "19a770df-0eb8-41c9-9e41-f2bdce0a74af"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e31f7d-1a90-46b3-beff-eb0b0c293f87",
      "metadata": {
        "id": "d7e31f7d-1a90-46b3-beff-eb0b0c293f87"
      },
      "source": [
        "## 5. Visualizing training and validation curves\n",
        "\n",
        "Using matplotlib, write code for two plots:\n",
        "1. Training and Validation loss\n",
        "2. Training and validation accuracy\n",
        "\n",
        "Each plot should show the epochs on the horizontal axis, the metric on the vertical axis, and a legend that shows a different color for the Training vs. Loss or Accuracy curves.\n",
        "\n",
        "These plots will be useful to visually display any:\n",
        "- Underfitting vs overfitting behavior.\n",
        "- The point at which early stopping kicks in.\n",
        "\n",
        "Make sure you have saved the results of your model's fit into 'history'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae27006b-7fd2-40f6-9dfc-6a6c0cfd8406",
      "metadata": {
        "id": "ae27006b-7fd2-40f6-9dfc-6a6c0cfd8406"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2019c90d-35ff-4161-8f5d-5d76bb3a25be",
      "metadata": {
        "id": "2019c90d-35ff-4161-8f5d-5d76bb3a25be"
      },
      "source": [
        "## Baseline: BiLSTM classifier performance\n",
        "\n",
        "Before we decouple the encoder and classifier, we can measure how well the full BiLSTM model performs on the test set. This gives a baseline:\n",
        "\n",
        "- Test accuracy\n",
        "- Classification report for Precision/recall/F1\n",
        "\n",
        "Later, we compare this to the FFClassifier trained on learned embeddings.  You can copy/adapt this code later and use it for other models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff53b0e-c322-450f-9e0a-d1a5761d5081",
      "metadata": {
        "id": "1ff53b0e-c322-450f-9e0a-d1a5761d5081"
      },
      "outputs": [],
      "source": [
        "# Predict probabilities with threshold at 0.5\n",
        "y_test_pred_proba = biLSTM.predict(X_test_seq)\n",
        "y_test_pred       = (y_test_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"BiLSTM model test accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"\\nClassification report (BiLSTM model):\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=[\"neg\", \"pos\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a1a14c-f441-4340-9d32-ec59ce90169e",
      "metadata": {
        "id": "09a1a14c-f441-4340-9d32-ec59ce90169e"
      },
      "source": [
        "## 6. Create sentence embeddings from the trained encoder\n",
        "\n",
        "Now use the **encoder part only** (up to and including the `sentence_embedding` layer) to generate fixed‑dimensional vectors for each review.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Extract the encoder inputs from:  `biLSTM.input`\n",
        "2. Extract the encoder outputs from: `biLSTM.get_layer(\"sentence_embedding\").output`\n",
        "3. Use the Keras `Model` class to construct a new model from those inputs and outputs, saving the model into a new variable, e.g. maybe you want to use 'encoder_only_model'.\n",
        "4. Use the 'predict' method of that new model on each of `X_train_seq`, `X_val_seq`, and `X_test_seq` to convert each movie review to a sentence embedding created by the bidirectional LSTM that was previously trained.\n",
        "5. Store those embeddings into new variables, e.g. X_train_emb for the training reviews.\n",
        "6. Call `encoder_only_model.summary()` (if that's the name you used) to verify the architecture\n",
        "7. Print the shapes of the three new variables where you saved the results of Step 5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810a309f-97e1-4387-8f09-33dcfa6771b6",
      "metadata": {
        "id": "810a309f-97e1-4387-8f09-33dcfa6771b6"
      },
      "outputs": [],
      "source": [
        "# Rebuild the encoder model that outputs the sentence embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49c398b-59e6-48fb-b8f1-2ee6ec276162",
      "metadata": {
        "id": "c49c398b-59e6-48fb-b8f1-2ee6ec276162"
      },
      "source": [
        "## 7. Define, compile and train a a new feed‑forward neural network (FFClassifier) on embeddings that you just saved.\n",
        "\n",
        "We now use the Keras to build a new **feed‑forward neural network** with 3 layers:\n",
        "\n",
        "- Input layer: to accept the sentence embeddings that you just created and saved.\n",
        "- Hidden layer: a Dense layer with a configurable number of units (your choice) and using \"relu\".\n",
        "- Output layer: a Dense layer with 1 unit and the \"sigmoid\" activation function.\n",
        "\n",
        "Compile this network as before, define EarlyStopping as before and fit the classifier, again saving it into 'history' or a different history variable of your choice.\n",
        "\n",
        "Make sure you use the embeddings you saved for the movie reviews for both training and validation.\n",
        "\n",
        "Predict the sentiment on the VALIDATION set, in a similar way (i.e. using 0.5 as the class threshold) that we predicted the test data the earlier model, but making sure you use the VALIDATION set embeddings that you just saved.\n",
        "\n",
        "Print the validation data accuracy and classification report, again as you have already done above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938391cf-ab4a-4f82-b425-f94bf97a57ec",
      "metadata": {
        "id": "938391cf-ab4a-4f82-b425-f94bf97a57ec"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7618844f-42e0-436f-9d67-5dda565598a6",
      "metadata": {
        "id": "7618844f-42e0-436f-9d67-5dda565598a6"
      },
      "source": [
        "## 8. Final training on train+validation and evaluation on test\n",
        "\n",
        "1. Stack the training and validation embeddings, i.e combine them into a single set of data.\n",
        "2. As you did near the beginning, create a new split of this data using train_test_split to create a new 90% split for training and 10% for validation, saving each into new variable names.\n",
        "3. Redefine and Retrain the FFClassifier on these training and validation datasets.\n",
        "4. Evaluate this final model on predictions for the held‑out test set, i.e. `X_test_emb` if that is the variable name you used for the test embedding data.\n",
        "5. Print your final accuracy and classifcation report.\n",
        "\n",
        "This gives us the final performance of the **BiLSTM embeddings + FFNN classifier** pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f69b8e4b-b743-401e-b5ed-4e44fef89cf1",
      "metadata": {
        "scrolled": true,
        "id": "f69b8e4b-b743-401e-b5ed-4e44fef89cf1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a78b69a6-887c-41ec-bde1-e54c68ac39d2",
      "metadata": {
        "id": "a78b69a6-887c-41ec-bde1-e54c68ac39d2"
      },
      "source": [
        "## 9. Confusion matrix for the Final classifier\n",
        "\n",
        "Using matplotlib (or just Seaborn's `heatmap`) plot a confusion matrix on that test data to show:\n",
        "- Which class is harder to predict.\n",
        "- Whether the model is biased toward positive or negative predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1716b176-fef9-4f40-97ef-1fd3014ef609",
      "metadata": {
        "id": "1716b176-fef9-4f40-97ef-1fd3014ef609"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f271c1f3-2779-4d46-a05d-2735fc018b7b",
      "metadata": {
        "id": "f271c1f3-2779-4d46-a05d-2735fc018b7b"
      },
      "source": [
        "## Error analysis: inspecting misclassified reviews\n",
        "\n",
        "We now examine some **false positives** and **false negatives** to understand:\n",
        "\n",
        "- Which linguistic phenomena confuse the model.\n",
        "- How the representation (BiLSTM embeddings) might fail.\n",
        "- How data sparsity, sarcasm, negation, etc. show up in errors.\n",
        "\n",
        "I am giving you this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5555313-3a95-46a4-a55a-a972f199586f",
      "metadata": {
        "id": "e5555313-3a95-46a4-a55a-a972f199586f"
      },
      "outputs": [],
      "source": [
        "# Identify misclassified indices\n",
        "\n",
        "y_test_pred = y_test_pred.ravel()\n",
        "misclassified_indices = np.where(y_test != y_test_pred.ravel())[0]\n",
        "print('Misclassified indices:\\n', misclassified_indices)\n",
        "\n",
        "print(\"\\nNumber of misclassified examples:\", len(misclassified_indices))\n",
        "\n",
        "# Helper function to print a few examples\n",
        "def show_misclassified_examples(X_text, y_true, y_pred, indices, n=5, truncated_text=True):\n",
        "    selected = np.random.choice(indices, size=min(n, len(indices)), replace=False)\n",
        "    for idx in selected:\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Index: {idx}\")\n",
        "        print(f\"True label: {y_true[idx]} ({'pos' if y_true[idx]==1 else 'neg'})\")\n",
        "        print(f\"Predicted label: {y_pred[idx]} ({'pos' if y_pred[idx]==1 else 'neg'})\")\n",
        "\n",
        "        if truncated_text:\n",
        "            print(\"\\nReview text (truncated to 800 chars):\\n\")\n",
        "            print(X_text[idx][:800])\n",
        "        else:\n",
        "            print(\"\\nReview full text:\\n\")\n",
        "            print(X_text[idx])\n",
        "        print(\"\\n\")\n",
        "\n",
        "truncated_text = True  # Truncate reviews to first 800 characters\n",
        "num_reviews    = 10     # Display this many reviews\n",
        "show_misclassified_examples(X_test, y_test, y_test_pred, misclassified_indices,\n",
        "                            n=num_reviews, truncated_text=truncated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "300fb6a1-f044-47d3-80ff-c34db65256d3",
      "metadata": {
        "id": "300fb6a1-f044-47d3-80ff-c34db65256d3"
      },
      "source": [
        "### 10. Use UMAP (imported at the beginning) to reduce the dimensionality of the test embeddings and plot them in 2 dimensions.\n",
        "\n",
        "Individual instructions for each code cell below:\n",
        "1. Print shape of test embeddings and accuracy score of the test data from Step 8 above.\n",
        "2. Define UMAP model to reduce X_test_emb (if that was your variable for the test embeddings) to 2 dimensions. Use 'cosine' as the metric, your own values  for n_neighbors, min_dist, and random_state. Call fit_transform method on the X test embeddings, and save result in a variable.  Print the new shape, which should be (400, 2).\n",
        "3. Create scatterplot of the ground truth labels (i.e. y_test) in the 2 dimensions with labeled axes, title, and legend. Color the plotted points by negative or positive sentiment.\n",
        "4. Create similar scatterplot to display plotted points that are either correct or incorrect predictions (You can use `y_test == y_test_pred` to get those)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefa4a6a-89cc-4a94-b9c3-8b85525e4760",
      "metadata": {
        "id": "eefa4a6a-89cc-4a94-b9c3-8b85525e4760"
      },
      "outputs": [],
      "source": [
        "# Print shape of test embeddings and accuracy score from Step 8\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039d0f52-ef1e-4869-b02d-7b18ff7c679a",
      "metadata": {
        "id": "039d0f52-ef1e-4869-b02d-7b18ff7c679a"
      },
      "outputs": [],
      "source": [
        "# Run UMAP to reduce embeddings to 2D\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2bc3e0c-cc15-4ee6-acc8-b7a9ebbbeb30",
      "metadata": {
        "scrolled": true,
        "id": "a2bc3e0c-cc15-4ee6-acc8-b7a9ebbbeb30"
      },
      "outputs": [],
      "source": [
        "# Scatterplot of ground-truth sentiment clusters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312412f9-f410-4ca9-8e81-879ffc2a405b",
      "metadata": {
        "id": "312412f9-f410-4ca9-8e81-879ffc2a405b"
      },
      "outputs": [],
      "source": [
        "# Compare with scatterplot of correct vs incorrect predictions\n",
        "\n",
        "correct = (y_test == y_test_pred)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c201957b-f045-4c5c-8031-2d56e8929807",
      "metadata": {
        "id": "c201957b-f045-4c5c-8031-2d56e8929807"
      },
      "source": [
        "Notice how the correct/incorrect predictions are  as thoroughtly mixed over the embedding space as the ground truth.\n",
        "\n",
        "This is because:\n",
        "\n",
        "- Only 2000 reviews total\n",
        "- Reviews are long, messy, and full of mixed sentiment\n",
        "- Many reviews contain both praise and criticism\n",
        "- The writing style varies wildly (sarcasm, rhetorical questions, plot summary)\n",
        "- Labels are sometimes ambiguous or borderline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56bb4979-01e0-462f-b786-862c1e9fa376",
      "metadata": {
        "id": "56bb4979-01e0-462f-b786-862c1e9fa376"
      },
      "source": [
        "#### Some topics for further thought.\n",
        "\n",
        "1. **Reducing overfitting:**\n",
        "   - Yaser Abu-Mostafa of CalTech says that learning how to deal with overfitting is what separates amateurs from professionals.\n",
        "   - If your model above seems to be overfitting, what do you think are the causes and how would you procees to reduce overfitting?\n",
        "\n",
        "2. **Embeddings as features:**\n",
        "   - In Assignment 1, you used N‑gram counts or TF–IDF as features.\n",
        "   - Here, you used **neural sentence embeddings**.\n",
        "   - In what sense are these embeddings \"just another kind of feature\"?  \n",
        "     In what sense are they fundamentally different?\n",
        "\n",
        "3. **Error patterns:**\n",
        "   - Look at some false positives and false negatives.\n",
        "   - What linguistic patterns seem especially hard for the model?\n",
        "   - How might you extend the model to handle these cases better (without jumping to Transformers yet)?\n",
        "\n",
        "4. **Bias–variance and data limitations:**\n",
        "   - How might model capacity (e.g., size of the BiLSTM or FFClassifier) affect underfitting vs overfitting here?\n",
        "   - If you had access to more data, what changes would you expect in performance and error patterns?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a42cca-d448-4ceb-b13c-3a9ac981d34d",
      "metadata": {
        "id": "d9a42cca-d448-4ceb-b13c-3a9ac981d34d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}